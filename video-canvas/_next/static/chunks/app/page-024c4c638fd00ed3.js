(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{2002:function(e,t,n){Promise.resolve().then(n.bind(n,8011))},8011:function(e,t,n){"use strict";n.d(t,{default:function(){return f}});var r=n(7437),o=n(4989),i=n.n(o),u=n(2265);async function a(e){let{canvasRef:t,videoRef:n,wgsl:r,frameRef:o}=e,i=t.current;if(!i)return null;let u=window.devicePixelRatio||1;if(i.width=i.clientWidth*u,i.height=i.clientHeight*u,!navigator.gpu)throw console.error("WebGPU not supported"),Error("WebGPU not supported on this browser!");let a=i.getContext("webgpu"),l=await navigator.gpu.requestAdapter();if(!l)throw console.error("No GPUAdapter"),Error("No appropriate GPUAdapter found.");let p=await l.requestDevice();if(!p)throw console.error("No device"),Error("Could not initialize webGPU canvas, no device");let c=p.createSampler({magFilter:"linear",minFilter:"linear",addressModeU:"clamp-to-edge",addressModeV:"clamp-to-edge"}),s=navigator.gpu.getPreferredCanvasFormat();a.configure({device:p,format:s});let d=new Float32Array([-1,-1,1,-1,1,1,-1,-1,1,1,-1,1]),f=p.createBuffer({label:"One Square vertices",size:d.byteLength,usage:GPUBufferUsage.VERTEX|GPUBufferUsage.COPY_DST});p.queue.writeBuffer(f,0,d);let g={arrayStride:8,attributes:[{format:"float32x2",offset:0,shaderLocation:0}]},m=p.createBuffer({size:4,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),v=new Float32Array([0]);p.queue.writeBuffer(m,0,v);let x=p.createBuffer({size:8,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),b=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,b);let y={binding:2,visibility:GPUShaderStage.FRAGMENT,sampler:{type:"filtering"}},h=p.createBindGroupLayout({label:"Bind Group Layout",entries:[{binding:0,visibility:GPUShaderStage.FRAGMENT,buffer:{}},{binding:1,visibility:GPUShaderStage.FRAGMENT,buffer:{}},y,{binding:3,visibility:GPUShaderStage.FRAGMENT,externalTexture:{}}]}),C=p.createShaderModule({label:"Simple shader module",code:r}),S=Date.now(),w=()=>{if(o.current.id=requestAnimationFrame(w),!n.current||n.current.readyState<2)return;try{p.importExternalTexture({source:n.current})}catch(e){return}let e=p.createBindGroup({layout:h,entries:[{binding:0,resource:{buffer:m}},{binding:1,resource:{buffer:x}},{binding:2,resource:c},{binding:3,resource:p.importExternalTexture({source:n.current})}]}),t=(Date.now()-S)/1e3;p.queue.writeBuffer(m,0,new Float32Array([t]));let r=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,r);let i=p.createPipelineLayout({label:"Pipeline Layout",bindGroupLayouts:[h]}),u=p.createRenderPipeline({label:"Simple shader pipeline",layout:i,vertex:{module:C,entryPoint:"vertexMain",buffers:[g]},fragment:{module:C,entryPoint:"fragmentMain",targets:[{format:s}]}}),l=p.createCommandEncoder(),v={view:a.getCurrentTexture().createView(),loadOp:"clear",storeOp:"store"},b=l.beginRenderPass({colorAttachments:[v]});b.setPipeline(u),b.setVertexBuffer(0,f),b.setBindGroup(0,e),b.draw(d.length/2),b.end(),p.queue.submit([l.finish()])};return w(),e=>{C=p.createShaderModule({label:"Simple shader module",code:e}),w()}}async function l(e){if(!e.current||!navigator.gpu)return;let t=e.current;try{let e=await navigator.mediaDevices.getUserMedia({video:!0});t.srcObject=e}catch(e){console.error("Failed to set up the webcam:",e)}}let p=e=>{let{canvasRef:t,videoRef:n,wgsl:r}=e,o=(0,u.useRef)({id:null}),i=(0,u.useRef)({initialized:!1,updateWgsl:null});(0,u.useEffect)(()=>(i.current.initialized?i.current.updateWgsl&&i.current.updateWgsl(r):(i.current.initialized=!0,a({canvasRef:t,videoRef:n,wgsl:r,frameRef:o}).then(e=>{i.current.updateWgsl=e}),l(n)),()=>{o.current.id&&cancelAnimationFrame(o.current.id)}),[t,n,r])},c={passthrough:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n    return texColor;\n}\n",greyscale:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Calculate the luminance from the RGB color components\n    let luminance = 0.299 * texColor.r + 0.587 * texColor.g + 0.114 * texColor.b;\n\n    // Create a greyscale color by setting all color components to the luminance value\n    return vec4f(luminance, luminance, luminance, 1.0);\n}\n",sepia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let sepiaColor = vec3f(\n        (texColor.r * 0.393) + (texColor.g * 0.769) + (texColor.b * 0.189),\n        (texColor.r * 0.349) + (texColor.g * 0.686) + (texColor.b * 0.168),\n        (texColor.r * 0.272) + (texColor.g * 0.534) + (texColor.b * 0.131)\n    );\n\n    return vec4f(sepiaColor, 1.0);\n}\n",edgeDetect:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Compute Sobel filter\n    let offset = vec2f(1.0) / resolution;\n\n    let left = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(offset.x, 0.0)).r;\n    let right = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(offset.x, 0.0)).r;\n    let top = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(0.0, offset.y)).r;\n    let bottom = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(0.0, offset.y)).r;\n\n    let edgeH = left - right;\n    let edgeV = top - bottom;\n\n    let edgeMagnitude = sqrt((edgeH * edgeH) + (edgeV * edgeV));\n\n    return vec4f(vec3f(edgeMagnitude), 1.0);\n}\n"},s=()=>(0,r.jsxs)("div",{children:[(0,r.jsx)("h2",{className:i().error_header,children:"Your browser does not support WebGPU."}),(0,r.jsxs)("p",{children:["Visit"," ",(0,r.jsx)("a",{target:"_blank",href:"https://caniuse.com/webgpu",children:"caniuse.com/webgpu"})," ","to see a list of currently supported web browsers."]})]}),d=e=>{let{filter:t,setFilter:n}=e;return(0,r.jsxs)("div",{className:i().filter_select_wrapper,children:[(0,r.jsx)("label",{htmlFor:"filter-select",children:"Current Filter: "}),(0,r.jsxs)("select",{id:"filter-select",value:t,onChange:e=>{n(e.target.value)},children:[(0,r.jsx)("option",{value:"passthrough",children:"Passthrough"}),(0,r.jsx)("option",{value:"greyscale",children:"Greyscale"}),(0,r.jsx)("option",{value:"sepia",children:"Sepia"}),(0,r.jsx)("option",{value:"edgeDetect",children:"Edge Detect"})]})]})};var f=()=>{let[e,t]=(0,u.useState)("passthrough"),n=(0,u.useRef)(null),o=(0,u.useRef)(null),[a,l]=(0,u.useState)(!0);return(0,u.useEffect)(()=>{l(null!=navigator.gpu)},[]),p({videoRef:n,canvasRef:o,wgsl:c[e]}),(0,r.jsxs)("div",{className:i().web_gpu_camera_wrapper,suppressHydrationWarning:!0,children:[(0,r.jsx)(d,{filter:e,setFilter:t}),(0,r.jsx)("video",{style:{visibility:"hidden",position:"absolute"},ref:n,autoPlay:!0,playsInline:!0,muted:!0}),(0,r.jsx)("canvas",{style:{width:"640px",height:"480px",margin:"20px"},ref:o}),a?null:(0,r.jsx)(s,{}),(0,r.jsx)("p",{className:i().privacy_notice,children:"None of the data from your webcam is ever transmitted or saved. The video processing takes place entirely in your browser. The webcamera is used as a video source to demonstrate that the video processing is happening in real-time."})]})}},4989:function(e){e.exports={web_gpu_camera_wrapper:"WebGpuCamera_web_gpu_camera_wrapper__qCTX9",privacy_notice:"WebGpuCamera_privacy_notice__F1DA6",error_header:"WebGpuCamera_error_header__k3JHU"}}},function(e){e.O(0,[700,971,23,744],function(){return e(e.s=2002)}),_N_E=e.O()}]);