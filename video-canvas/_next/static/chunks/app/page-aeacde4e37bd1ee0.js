(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{2002:function(e,t,n){Promise.resolve().then(n.bind(n,4093))},4093:function(e,t,n){"use strict";n.d(t,{default:function(){return d}});var r=n(7437),o=n(4989),i=n.n(o),u=n(2265);async function a(e){let{canvasRef:t,videoRef:n,wgsl:r,frameRef:o}=e,i=t.current;if(!i)return null;let u=window.devicePixelRatio||1;if(i.width=i.clientWidth*u,i.height=i.clientHeight*u,!navigator.gpu)throw console.error("WebGPU not supported"),Error("WebGPU not supported on this browser!");let a=i.getContext("webgpu"),l=await navigator.gpu.requestAdapter();if(!l)throw console.error("No GPUAdapter"),Error("No appropriate GPUAdapter found.");let p=await l.requestDevice();if(!p)throw console.error("No device"),Error("Could not initialize webGPU canvas, no device");let s=p.createSampler({magFilter:"linear",minFilter:"linear",addressModeU:"clamp-to-edge",addressModeV:"clamp-to-edge"}),c=navigator.gpu.getPreferredCanvasFormat();a.configure({device:p,format:c});let f=new Float32Array([-1,-1,1,-1,1,1,-1,-1,1,1,-1,1]),d=p.createBuffer({label:"One Square vertices",size:f.byteLength,usage:GPUBufferUsage.VERTEX|GPUBufferUsage.COPY_DST});p.queue.writeBuffer(d,0,f);let m={arrayStride:8,attributes:[{format:"float32x2",offset:0,shaderLocation:0}]},g=p.createBuffer({size:4,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),v=new Float32Array([0]);p.queue.writeBuffer(g,0,v);let x=p.createBuffer({size:8,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),y=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,y);let b={binding:2,visibility:GPUShaderStage.FRAGMENT,sampler:{type:"filtering"}},h=p.createBindGroupLayout({label:"Bind Group Layout",entries:[{binding:0,visibility:GPUShaderStage.FRAGMENT,buffer:{}},{binding:1,visibility:GPUShaderStage.FRAGMENT,buffer:{}},b,{binding:3,visibility:GPUShaderStage.FRAGMENT,externalTexture:{}}]}),C=p.createShaderModule({label:"Simple shader module",code:r}),S=Date.now(),w=()=>{if(o.current.id=requestAnimationFrame(w),!n.current||n.current.readyState<2)return;try{p.importExternalTexture({source:n.current})}catch(e){return}let e=p.createBindGroup({layout:h,entries:[{binding:0,resource:{buffer:g}},{binding:1,resource:{buffer:x}},{binding:2,resource:s},{binding:3,resource:p.importExternalTexture({source:n.current})}]}),t=(Date.now()-S)/1e3;p.queue.writeBuffer(g,0,new Float32Array([t]));let r=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,r);let i=p.createPipelineLayout({label:"Pipeline Layout",bindGroupLayouts:[h]}),u=p.createRenderPipeline({label:"Simple shader pipeline",layout:i,vertex:{module:C,entryPoint:"vertexMain",buffers:[m]},fragment:{module:C,entryPoint:"fragmentMain",targets:[{format:c}]}}),l=p.createCommandEncoder(),v={view:a.getCurrentTexture().createView(),loadOp:"clear",storeOp:"store"},y=l.beginRenderPass({colorAttachments:[v]});y.setPipeline(u),y.setVertexBuffer(0,d),y.setBindGroup(0,e),y.draw(f.length/2),y.end(),p.queue.submit([l.finish()])};return w(),e=>{C=p.createShaderModule({label:"Simple shader module",code:e}),w()}}async function l(e){if(!e.current||!navigator.gpu)return;let t=e.current;try{let e=await navigator.mediaDevices.getUserMedia({video:!0});t.srcObject=e}catch(e){console.error("Failed to set up the webcam:",e)}}let p=e=>{let{canvasRef:t,videoRef:n,wgsl:r}=e,o=(0,u.useRef)({id:null}),i=(0,u.useRef)({initialized:!1,updateWgsl:null});(0,u.useEffect)(()=>(i.current.initialized?i.current.updateWgsl&&i.current.updateWgsl(r):(i.current.initialized=!0,a({canvasRef:t,videoRef:n,wgsl:r,frameRef:o}).then(e=>{i.current.updateWgsl=e}),l(n)),()=>{o.current.id&&cancelAnimationFrame(o.current.id)}),[t,n,r])},s={passthrough:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n    return texColor;\n}\n",greyscale:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Calculate the luminance from the RGB color components\n    let luminance = 0.299 * texColor.r + 0.587 * texColor.g + 0.114 * texColor.b;\n\n    // Create a greyscale color by setting all color components to the luminance value\n    return vec4f(luminance, luminance, luminance, 1.0);\n}\n",sepia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let sepiaColor = vec3f(\n        (texColor.r * 0.393) + (texColor.g * 0.769) + (texColor.b * 0.189),\n        (texColor.r * 0.349) + (texColor.g * 0.686) + (texColor.b * 0.168),\n        (texColor.r * 0.272) + (texColor.g * 0.534) + (texColor.b * 0.131)\n    );\n\n    return vec4f(sepiaColor, 1.0);\n}\n",invert:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let invertedColor = vec4f(1.0 - texColor.rgb, texColor.a);\n\n    return invertedColor;\n}\n",gaussianBlur:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let offset = vec2f(1.0) / resolution;\n    let weights = array<f32, 9>(0.204164, 0.304005, 0.093913, 0.023418, 0.001298, 0.001298, 0.023418, 0.093913, 0.304005);\n    let weightSum = weights[0] + 2.0 * (weights[1] + weights[2] + weights[3] + weights[4] + weights[5] + weights[6] + weights[7] + weights[8]);\n\n    var blurColor = texColor * weights[0];\n    for (var i = 1u; i < 9u; i = i + 1u) {\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(offset.x * f32(i), 0.0)) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(offset.x * f32(i), 0.0)) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(0.0, offset.y * f32(i))) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(0.0, offset.y * f32(i))) * weights[i];\n    }\n\n    blurColor = blurColor / weightSum; // Normalize the result\n    blurColor = blurColor * 0.9; // Slightly darken the result\n\n    return blurColor;\n}\n",edgeDetect:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Compute Sobel filter\n    let offset = vec2f(1.0) / resolution;\n\n    let left = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(offset.x, 0.0)).r;\n    let right = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(offset.x, 0.0)).r;\n    let top = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(0.0, offset.y)).r;\n    let bottom = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(0.0, offset.y)).r;\n\n    let edgeH = left - right;\n    let edgeV = top - bottom;\n\n    let edgeMagnitude = sqrt((edgeH * edgeH) + (edgeV * edgeV));\n\n    return vec4f(vec3f(edgeMagnitude), 1.0);\n}\n"},c=()=>(0,r.jsxs)("div",{children:[(0,r.jsx)("h2",{className:i().error_header,children:"Your browser does not support WebGPU."}),(0,r.jsxs)("p",{children:["Visit"," ",(0,r.jsx)("a",{target:"_blank",href:"https://caniuse.com/webgpu",children:"caniuse.com/webgpu"})," ","to see a list of currently supported web browsers."]})]}),f=e=>{let{filter:t,setFilter:n}=e;return(0,r.jsxs)("div",{className:i().filter_select_wrapper,children:[(0,r.jsx)("label",{htmlFor:"filter-select",children:"Current Filter: "}),(0,r.jsxs)("select",{id:"filter-select",value:t,onChange:e=>{n(e.target.value)},children:[(0,r.jsx)("option",{value:"passthrough",children:"Passthrough"}),(0,r.jsx)("option",{value:"greyscale",children:"Greyscale"}),(0,r.jsx)("option",{value:"sepia",children:"Sepia"}),(0,r.jsx)("option",{value:"invert",children:"Invert Colors"}),(0,r.jsx)("option",{value:"gaussianBlur",children:"Gaussian Blur"}),(0,r.jsx)("option",{value:"edgeDetect",children:"Edge Detect"})]})]})};var d=()=>{let[e,t]=(0,u.useState)("passthrough"),n=(0,u.useRef)(null),o=(0,u.useRef)(null),[a,l]=(0,u.useState)(!0);return(0,u.useEffect)(()=>{l(null!=navigator.gpu)},[]),p({videoRef:n,canvasRef:o,wgsl:s[e]}),(0,r.jsxs)("div",{className:i().web_gpu_camera_wrapper,suppressHydrationWarning:!0,children:[(0,r.jsx)(f,{filter:e,setFilter:t}),(0,r.jsx)("video",{style:{visibility:"hidden",position:"absolute"},ref:n,autoPlay:!0,playsInline:!0,muted:!0}),(0,r.jsx)("canvas",{style:{width:"640px",height:"480px",margin:"20px"},ref:o}),a?null:(0,r.jsx)(c,{}),(0,r.jsx)("p",{className:i().privacy_notice,children:"None of the data from your webcam is ever transmitted or saved. The video processing takes place entirely in your browser. The webcamera is used as a video source to demonstrate that the video processing is happening in real-time."})]})}},4989:function(e){e.exports={web_gpu_camera_wrapper:"WebGpuCamera_web_gpu_camera_wrapper__qCTX9",privacy_notice:"WebGpuCamera_privacy_notice__F1DA6",error_header:"WebGpuCamera_error_header__k3JHU"}}},function(e){e.O(0,[700,971,23,744],function(){return e(e.s=2002)}),_N_E=e.O()}]);