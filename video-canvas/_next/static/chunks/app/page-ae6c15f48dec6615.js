(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{2002:function(e,n,t){Promise.resolve().then(t.bind(t,814))},814:function(e,n,t){"use strict";t.d(n,{default:function(){return f}});var r=t(7437),o=t(4989),u=t.n(o),a=t(2265);async function i(e){let{canvasRef:n,videoRef:t,wgsl:r,frameRef:o}=e,u=n.current;if(!u)return null;let a=window.devicePixelRatio||1;if(u.width=u.clientWidth*a,u.height=u.clientHeight*a,!navigator.gpu)throw console.error("WebGPU not supported"),Error("WebGPU not supported on this browser!");let i=u.getContext("webgpu"),l=await navigator.gpu.requestAdapter();if(!l)throw console.error("No GPUAdapter"),Error("No appropriate GPUAdapter found.");let p=await l.requestDevice();if(!p)throw console.error("No device"),Error("Could not initialize webGPU canvas, no device");let c=p.createSampler({magFilter:"linear",minFilter:"linear",addressModeU:"clamp-to-edge",addressModeV:"clamp-to-edge"}),s=navigator.gpu.getPreferredCanvasFormat();i.configure({device:p,format:s});let v=new Float32Array([-1,-1,1,-1,1,1,-1,-1,1,1,-1,1]),f=p.createBuffer({label:"One Square vertices",size:v.byteLength,usage:GPUBufferUsage.VERTEX|GPUBufferUsage.COPY_DST});p.queue.writeBuffer(f,0,v);let m={arrayStride:8,attributes:[{format:"float32x2",offset:0,shaderLocation:0}]},d=p.createBuffer({size:4,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),g=new Float32Array([0]);p.queue.writeBuffer(d,0,g);let x=p.createBuffer({size:8,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),C=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,C);let y={binding:2,visibility:GPUShaderStage.FRAGMENT,sampler:{type:"filtering"}},b=p.createBindGroupLayout({label:"Bind Group Layout",entries:[{binding:0,visibility:GPUShaderStage.FRAGMENT,buffer:{}},{binding:1,visibility:GPUShaderStage.FRAGMENT,buffer:{}},y,{binding:3,visibility:GPUShaderStage.FRAGMENT,externalTexture:{}}]}),h=p.createShaderModule({label:"Simple shader module",code:r}),S=Date.now(),w=()=>{if(o.current.id=requestAnimationFrame(w),!t.current||t.current.readyState<2)return;try{p.importExternalTexture({source:t.current})}catch(e){return}let e=p.createBindGroup({layout:b,entries:[{binding:0,resource:{buffer:d}},{binding:1,resource:{buffer:x}},{binding:2,resource:c},{binding:3,resource:p.importExternalTexture({source:t.current})}]}),n=(Date.now()-S)/1e3;p.queue.writeBuffer(d,0,new Float32Array([n]));let r=new Float32Array([window.innerWidth,window.innerHeight]);p.queue.writeBuffer(x,0,r);let u=p.createPipelineLayout({label:"Pipeline Layout",bindGroupLayouts:[b]}),a=p.createRenderPipeline({label:"Simple shader pipeline",layout:u,vertex:{module:h,entryPoint:"vertexMain",buffers:[m]},fragment:{module:h,entryPoint:"fragmentMain",targets:[{format:s}]}}),l=p.createCommandEncoder(),g={view:i.getCurrentTexture().createView(),loadOp:"clear",storeOp:"store"},C=l.beginRenderPass({colorAttachments:[g]});C.setPipeline(a),C.setVertexBuffer(0,f),C.setBindGroup(0,e),C.draw(v.length/2),C.end(),p.queue.submit([l.finish()])};return w(),e=>{h=p.createShaderModule({label:"Simple shader module",code:e}),w()}}async function l(e){if(!e.current||!navigator.gpu)return;let n=e.current;try{let e=await navigator.mediaDevices.getUserMedia({video:!0});n.srcObject=e}catch(e){console.error("Failed to set up the webcam:",e)}}let p=e=>{let{canvasRef:n,videoRef:t,wgsl:r}=e,o=(0,a.useRef)({id:null}),u=(0,a.useRef)({initialized:!1,updateWgsl:null});(0,a.useEffect)(()=>(u.current.initialized?u.current.updateWgsl&&u.current.updateWgsl(r):(u.current.initialized=!0,i({canvasRef:n,videoRef:t,wgsl:r,frameRef:o}).then(e=>{u.current.updateWgsl=e}),l(t)),()=>{o.current.id&&cancelAnimationFrame(o.current.id)}),[n,t,r])},c={passthrough:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n    return texColor;\n}\n",greyscale:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Calculate the luminance from the RGB color components\n    let luminance = 0.299 * texColor.r + 0.587 * texColor.g + 0.114 * texColor.b;\n\n    // Create a greyscale color by setting all color components to the luminance value\n    return vec4f(luminance, luminance, luminance, 1.0);\n}\n",sepia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let sepiaColor = vec3f(\n        (texColor.r * 0.393) + (texColor.g * 0.769) + (texColor.b * 0.189),\n        (texColor.r * 0.349) + (texColor.g * 0.686) + (texColor.b * 0.168),\n        (texColor.r * 0.272) + (texColor.g * 0.534) + (texColor.b * 0.131)\n    );\n\n    return vec4f(sepiaColor, 1.0);\n}\n",invert:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let invertedColor = vec4f(1.0 - texColor.rgb, texColor.a);\n\n    return invertedColor;\n}\n",gaussianBlur:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    let offset = vec2f(1.0) / resolution;\n    let weights = array<f32, 9>(0.204164, 0.304005, 0.093913, 0.023418, 0.001298, 0.001298, 0.023418, 0.093913, 0.304005);\n    let weightSum = weights[0] + 2.0 * (weights[1] + weights[2] + weights[3] + weights[4] + weights[5] + weights[6] + weights[7] + weights[8]);\n\n    var blurColor = texColor * weights[0];\n    for (var i = 1u; i < 9u; i = i + 1u) {\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(offset.x * f32(i), 0.0)) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(offset.x * f32(i), 0.0)) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(0.0, offset.y * f32(i))) * weights[i];\n        blurColor += textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(0.0, offset.y * f32(i))) * weights[i];\n    }\n\n    blurColor = blurColor / weightSum; // Normalize the result\n    blurColor = blurColor * 0.9; // Slightly darken the result\n\n    return blurColor;\n}\n",edgeDetect:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5; // Normalized UV coordinates\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Compute Sobel filter\n    let offset = vec2f(1.0) / resolution;\n\n    let left = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(offset.x, 0.0)).r;\n    let right = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(offset.x, 0.0)).r;\n    let top = textureSampleBaseClampToEdge(myTexture, mySampler, uv - vec2f(0.0, offset.y)).r;\n    let bottom = textureSampleBaseClampToEdge(myTexture, mySampler, uv + vec2f(0.0, offset.y)).r;\n\n    let edgeH = left - right;\n    let edgeV = top - bottom;\n\n    let edgeMagnitude = sqrt((edgeH * edgeH) + (edgeV * edgeV));\n\n    return vec4f(vec3f(edgeMagnitude), 1.0);\n}\n",protanopia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\nfn applyContrast(color: vec3f, contrast: f32) -> vec3f {\n    return (color - 0.5) * contrast + 0.5;\n}\n\nfn increaseSaturation(color: vec3f, amount: f32) -> vec3f {\n    let gray = dot(color, vec3f(0.299, 0.587, 0.114)); // Luminance\n    return mix(vec3f(gray), color, amount);\n}\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5;\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Protanopia color adjustment\n    var newRed = 0.567 * texColor.r + 0.433 * texColor.g;\n    var newGreen = 0.558 * texColor.r + 0.442 * texColor.g;\n    var newBlue = texColor.b;\n\n    // Apply contrast enhancement\n    let contrast = 1.5; // Higher contrast factor\n    let enhancedColor = applyContrast(vec3f(newRed, newGreen, newBlue), contrast);\n\n    // Increase saturation\n    let saturatedColor = increaseSaturation(enhancedColor, 1.5); // High saturation amount\n\n    return vec4f(saturatedColor, texColor.a);\n}\n",deuteranopia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\nfn applyContrast(color: vec3f, contrast: f32) -> vec3f {\n    return (color - 0.5) * contrast + 0.5;\n}\n\nfn increaseSaturation(color: vec3f, amount: f32) -> vec3f {\n    let gray = dot(color, vec3f(0.299, 0.587, 0.114)); // Luminance\n    return mix(vec3f(gray), color, amount);\n}\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5;\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Deuteranopia color adjustment\n    var newRed = 0.625 * texColor.r + 0.375 * texColor.g;\n    var newGreen = 0.7 * texColor.r + 0.3 * texColor.g;\n    var newBlue = texColor.b;\n\n    // Apply contrast enhancement\n    let contrast = 1.5; // Higher contrast factor\n    let enhancedColor = applyContrast(vec3f(newRed, newGreen, newBlue), contrast);\n\n    // Increase saturation\n    let saturatedColor = increaseSaturation(enhancedColor, 1.5); // High saturation amount\n\n    return vec4f(saturatedColor, texColor.a);\n}\n",tritanopia:"struct VertexInput {\n    @location(0) pos: vec2f,\n}\n\nstruct VertexOutput {\n    @builtin(position) position: vec4f,\n    @location(1) clipSpaceCoord: vec2f,\n}\n\nstruct FragmentInput {\n    @location(1) clipSpaceCoord: vec2f,\n}\n\n@vertex\nfn vertexMain(input: VertexInput) -> VertexOutput {\n    var output: VertexOutput;\n    output.position = vec4f(input.pos, 0.0, 1.0);\n    output.clipSpaceCoord = input.pos;\n    return output;\n}\n\n@group(0) @binding(0) var<uniform> time: f32;\n@group(0) @binding(1) var<uniform> resolution: vec2f;\n@group(0) @binding(2) var mySampler: sampler;\n@group(0) @binding(3) var myTexture: texture_external;\n\nfn applyContrast(color: vec3f, contrast: f32) -> vec3f {\n    return (color - 0.5) * contrast + 0.5;\n}\n\nfn increaseSaturation(color: vec3f, amount: f32) -> vec3f {\n    let gray = dot(color, vec3f(0.299, 0.587, 0.114)); // Luminance\n    return mix(vec3f(gray), color, amount);\n}\n\n@fragment\nfn fragmentMain(input: FragmentInput) -> @location(0) vec4f {\n    var uv = input.clipSpaceCoord.xy / 2.0 + 0.5;\n    uv.x = 1.0 - uv.x;\n    uv.y = 1.0 - uv.y;\n    let texColor = textureSampleBaseClampToEdge(myTexture, mySampler, uv);\n\n    // Tritanopia color adjustment\n    var newRed = texColor.r;\n    var newGreen = 0.8 * texColor.g + 0.2 * texColor.b;\n    var newBlue = 0.258 * texColor.g + 0.742 * texColor.b;\n\n    // Apply contrast enhancement\n    let contrast = 1.5; // Higher contrast factor\n    let enhancedColor = applyContrast(vec3f(newRed, newGreen, newBlue), contrast);\n\n    // Increase saturation\n    let saturatedColor = increaseSaturation(enhancedColor, 1.5); // High saturation amount\n\n    return vec4f(saturatedColor, texColor.a);\n}\n"},s=()=>(0,r.jsxs)("div",{children:[(0,r.jsx)("h2",{className:u().error_header,children:"Your browser does not support WebGPU."}),(0,r.jsxs)("p",{children:["Visit"," ",(0,r.jsx)("a",{target:"_blank",href:"https://caniuse.com/webgpu",children:"caniuse.com/webgpu"})," ","to see a list of currently supported web browsers."]})]}),v=e=>{let{filter:n,setFilter:t}=e;return(0,r.jsxs)("div",{className:u().filter_select_wrapper,children:[(0,r.jsx)("label",{htmlFor:"filter-select",children:"Current Filter: "}),(0,r.jsxs)("select",{id:"filter-select",value:n,onChange:e=>{t(e.target.value)},children:[(0,r.jsx)("option",{value:"passthrough",children:"Passthrough"}),(0,r.jsx)("option",{value:"greyscale",children:"Greyscale"}),(0,r.jsx)("option",{value:"sepia",children:"Sepia"}),(0,r.jsx)("option",{value:"invert",children:"Invert Colors"}),(0,r.jsx)("option",{value:"gaussianBlur",children:"Gaussian Blur"}),(0,r.jsx)("option",{value:"edgeDetect",children:"Edge Detect"}),(0,r.jsx)("option",{value:"protanopia",children:"Protanopia Correction"}),(0,r.jsx)("option",{value:"deuteranopia",children:"Deuteranopia Correction"}),(0,r.jsx)("option",{value:"tritanopia",children:"Tritanopia Correction"})]})]})};var f=()=>{let[e,n]=(0,a.useState)("passthrough"),t=(0,a.useRef)(null),o=(0,a.useRef)(null),[i,l]=(0,a.useState)(!0);return(0,a.useEffect)(()=>{l(null!=navigator.gpu)},[]),p({videoRef:t,canvasRef:o,wgsl:c[e]}),(0,r.jsxs)("div",{className:u().web_gpu_camera_wrapper,suppressHydrationWarning:!0,children:[(0,r.jsx)(v,{filter:e,setFilter:n}),(0,r.jsx)("video",{style:{visibility:"hidden",position:"absolute"},ref:t,autoPlay:!0,playsInline:!0,muted:!0}),(0,r.jsx)("canvas",{style:{width:"640px",height:"480px",margin:"20px"},ref:o}),i?null:(0,r.jsx)(s,{}),(0,r.jsx)("p",{className:u().privacy_notice,children:"None of the data from your webcam is ever transmitted or saved. The video processing takes place entirely in your browser. The webcamera is used as a video source to demonstrate that the video processing is happening in real-time."})]})}},4989:function(e){e.exports={web_gpu_camera_wrapper:"WebGpuCamera_web_gpu_camera_wrapper__qCTX9",privacy_notice:"WebGpuCamera_privacy_notice__F1DA6",error_header:"WebGpuCamera_error_header__k3JHU"}}},function(e){e.O(0,[700,971,23,744],function(){return e(e.s=2002)}),_N_E=e.O()}]);